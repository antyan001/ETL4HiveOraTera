{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "curruser = os.environ.get('USER')\n",
    "\n",
    "_labdata = os.environ.get(\"LABDATA_PYSPARK\")\n",
    "sys.path.insert(0, _labdata)\n",
    "os.chdir(_labdata)\n",
    "\n",
    "if curruser in os.listdir(\"/opt/workspace/\"):\n",
    "    sys.path.insert(0, '/opt/workspace/{user}/notebooks/support_library/'.format(user=curruser))\n",
    "    sys.path.insert(0, '/opt/workspace/{user}/libs/python3.5/site-packages/'.format(user=curruser))\n",
    "    # sys.path.insert(0, '/opt/workspace/{user}/notebooks/labdata_v1.2/lib/'.format(user=curruser))\n",
    "else:\n",
    "    sys.path.insert(0, '/home/{}/notebooks/support_library/'.format(curruser))\n",
    "    sys.path.insert(0, '/home/{}/python35-libs/lib/python3.5/site-packages/'.format(curruser))\n",
    "    # sys.path.insert(0, '/home/{}/notebooks/labdata/lib/'.format(curruser))\n",
    "\n",
    "#import tendo.singleton\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import joblib\n",
    "import json\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from time import sleep\n",
    "from itertools import islice\n",
    "from multiprocessing import Pool, Process, JoinableQueue\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from functools import partial\n",
    "import subprocess\n",
    "from threading import Thread\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from transliterate import translit\n",
    "\n",
    "from lib.spark_connector import SparkConnector\n",
    "from lib.sparkdb_loader import *\n",
    "from lib.connector import OracleDB\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf, HiveContext\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import loader as load\n",
    "from collections import ChainMap\n",
    "\n",
    "from lib.config import *\n",
    "from lib.tools import *\n",
    "\n",
    "# sing = tendo.singleton.SingleInstance()\n",
    "\n",
    "# os.chdir('/opt/workspace/ektov/notebooks/Clickstream_Analytics/AutoUpdate/')\n",
    "# os.chdir('/opt/workspace/{}/notebooks/clickstream/AutoUpdate/'.format(curruser))\n",
    "\n",
    "def show(self, n=10):\n",
    "    return self.limit(n).toPandas()\n",
    "\n",
    "def typed_udf(return_type):\n",
    "    '''Make a UDF decorator with the given return type'''\n",
    "\n",
    "    def _typed_udf_wrapper(func):\n",
    "        return f.udf(func,return_type)\n",
    "\n",
    "    return _typed_udf_wrapper\n",
    "\n",
    "pyspark.sql.dataframe.DataFrame.show = show\n",
    "\n",
    "def print_and_log(message: str):\n",
    "    print(message)\n",
    "    logger.info(message)\n",
    "    return None\n",
    "\n",
    "CONN_SCHEMA = 'sbx_team_digitcamp' #'sbx_t_team_cvm'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================== 2021-09-14 ======================================================\n",
      "# __init__ : begin\n",
      "2.4.0.cloudera2\n"
     ]
    }
   ],
   "source": [
    "sp = spark(schema=CONN_SCHEMA,\n",
    "               dynamic_alloc=False,\n",
    "               numofinstances=10,\n",
    "               numofcores=8,\n",
    "               executor_memory='35g',\n",
    "               driver_memory='35g',\n",
    "               kerberos_auth=True,\n",
    "               process_label=\"TEST_PYSPARK_\"\n",
    "               )\n",
    "\n",
    "hive = sp.sql\n",
    "print(sp.sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hive.setConf(\"hive.exec.dynamic.partition\",\"true\")\n",
    "hive.setConf(\"hive.exec.dynamic.partition.mode\",\"nonstrict\")\n",
    "hive.setConf(\"hive.enforce.bucketing\",\"false\")\n",
    "hive.setConf(\"hive.enforce.sorting\",\"false\")\n",
    "hive.setConf(\"spark.sql.sources.partitionOverwiteMode\",\"dynamic\")\n",
    "# hive.setConf(\"hive.exec.stagingdir\", \"/tmp/{}/\".format(curruser))\n",
    "# hive.setConf(\"hive.exec.scratchdir\", \"/tmp/{}/\".format(curruser))\n",
    "hive.setConf(\"hive.load.dynamic.partitions.thread\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sdf = hive.table('sbx_team_digitcamp.cid_inn_insert_2021_09_10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Partitioned SDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createSDF(conn_schema, target_tbl, insert, part_tupl_lst):\n",
    "\n",
    "    hive.sql('''create table {schema}.{tbl} (\n",
    "                                             {fields}\n",
    "                                                )\n",
    "                 PARTITIONED BY ({part_col_lst})\n",
    "             '''.format(schema=conn_schema,\n",
    "                        tbl=target_tbl,\n",
    "                        fields=insert,\n",
    "                        part_col_lst=part_tupl_lst)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def insertToSDF(sdf, conn_schema, tmp_tbl, target_tbl, part_col_lst):\n",
    "    \n",
    "    sdf.registerTempTable(tmp_tbl)\n",
    "    \n",
    "    hive.sql(\"\"\"\n",
    "    insert overwrite table {schema}.{tbl}\n",
    "    partition({part_col})\n",
    "    select * from {tmp_tbl}\n",
    "    distribute by ({part_col})\n",
    "    \"\"\".format(schema=conn_schema,\n",
    "               tbl=target_tbl,\n",
    "               tmp_tbl=tmp_tbl,\n",
    "               part_col=part_col_lst)\n",
    "            )\n",
    "\n",
    "def collectRowsByIndex(i, it, indxs):\n",
    "    out = []\n",
    "    if i in indxs:\n",
    "         out.extend(list(it)) #islice(it,0,5) \n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn_schema = 'sbx_team_digitcamp'\n",
    "table_name = 'ga_cid_sbbol_inn_update'\n",
    "part_tupl_lst = [('ctl_loading', 'bigint')]\n",
    "part_tupl_str = ', '.join([\"{} {}\".format(col, _type) for col, _type in part_tupl_lst])\n",
    "                          \n",
    "hive.sql(\"drop table if exists {schema}.{tbl} purge\".format(schema=conn_schema, tbl=table_name))\n",
    "insert = ', '.join([\"{} {}\".format(col, _type) for col, _type in sdf.dtypes if col.lower() not in part_tupl_lst[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "createSDF(conn_schema, target_tbl=table_name, insert=insert, part_tupl_lst=part_tupl_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map Whole SDF into Partitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "insertToSDF(sdf,\n",
    "            conn_schema='sbx_team_digitcamp',\n",
    "            tmp_tbl='tmp_ga_cid_inn', \n",
    "            target_tbl='ga_cid_sbbol_inn_update', \n",
    "            part_col_lst='ctl_loading')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXchange Partitions Between Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for part in part_diff:\n",
    "#     hive.sql('''ALTER TABLE {schema}.{tbl0} EXCHANGE PARTITION (ctl_loading='{prt}') WITH TABLE {schema}.{tbl1}'''\\\n",
    "#              .format(schema=conn_schema, \n",
    "#                      tbl0 ='ga_cid_sbbol_inn_update',      \n",
    "#                      tbl1 ='ga_cid_sbbol_inn',\n",
    "#                      prt=part\n",
    "#                     )\n",
    "#             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Two Partitioned Tables via hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = 'hdfs://clsklsbx/user/team/team_digitcamp/hive/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parts_from = hive.sql(\"show partitions {}.{}\".format('sbx_team_digitcamp','ga_cid_sbbol_inn_update')).collect()\n",
    "parts_from = [part for part in parts_from if not part['partition'].endswith('__HIVE_DEFAULT_PARTITION__')]\n",
    "parts_from = sorted(parts_from,reverse=True, key=lambda x: int(x['partition'].split('=')[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parts_from = [part['partition'] for part in parts_from if not part['partition'].endswith('__HIVE_DEFAULT_PARTITION__')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts_to = hive.sql(\"show partitions {}.{}\".format('sbx_team_digitcamp','ga_cid_sbbol_inn')).collect()\n",
    "parts_to = [part for part in parts_to if not part['partition'].endswith('__HIVE_DEFAULT_PARTITION__')]\n",
    "parts_to = sorted(parts_to, reverse=True, key=lambda x: int(x['partition'].split('=')[-1]))\n",
    "# parts_to = sorted(parts_to,reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "part_diff = set(parts_from) - set(parts_to)\n",
    "part_diff = [part.split('=')[-1]  for part in part_diff]\n",
    "part_diff = sorted(part_diff,reverse=True)\n",
    "part_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADDING PARTITION: 11482743...\n",
      "ADDING PARTITION: 11481779...\n",
      "ADDING PARTITION: 11461830...\n",
      "ADDING PARTITION: 11461026...\n",
      "ADDING PARTITION: 11459968...\n",
      "ADDING PARTITION: 11441601...\n",
      "ADDING PARTITION: 11422415...\n",
      "ADDING PARTITION: 11404396...\n",
      "ADDING PARTITION: 11403618...\n",
      "ADDING PARTITION: 11390067...\n",
      "ADDING PARTITION: 11389420...\n",
      "ADDING PARTITION: 11388762...\n",
      "ADDING PARTITION: 11373411...\n",
      "ADDING PARTITION: 11372608...\n",
      "ADDING PARTITION: 11371633...\n",
      "ADDING PARTITION: 11354808...\n",
      "ADDING PARTITION: 11353676...\n",
      "ADDING PARTITION: 11352959...\n",
      "ADDING PARTITION: 11352321...\n",
      "ADDING PARTITION: 11351272...\n",
      "ADDING PARTITION: 11332912...\n",
      "ADDING PARTITION: 11332186...\n",
      "ADDING PARTITION: 11331229...\n",
      "ADDING PARTITION: 11311484...\n",
      "ADDING PARTITION: 11292575...\n",
      "ADDING PARTITION: 11291830...\n",
      "ADDING PARTITION: 11275461...\n",
      "ADDING PARTITION: 11274756...\n",
      "ADDING PARTITION: 11273921...\n",
      "ADDING PARTITION: 11259526...\n",
      "ADDING PARTITION: 11259276...\n",
      "ADDING PARTITION: 11241503...\n",
      "ADDING PARTITION: 11240862...\n",
      "ADDING PARTITION: 11239743...\n",
      "ADDING PARTITION: 11221260...\n",
      "ADDING PARTITION: 11220586...\n"
     ]
    }
   ],
   "source": [
    "for part_num in part_diff:\n",
    "    print('ADDING PARTITION: {}...'.format(part_num))\n",
    "    hive.sql('''ALTER TABLE sbx_team_digitcamp.ga_cid_sbbol_inn ADD IF NOT EXISTS PARTITION(ctl_loading='{}')'''.format(part_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "205349c10dcd41a68e9ba8b7481b2e81"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for ctl in tqdm_notebook(parts_from, total=len(parts_from)):\n",
    "    \n",
    "    hdfs_from = data_path+'ga_cid_sbbol_inn_update'+'/'+'{}/*'.format(ctl)\n",
    "    hdfs_to   = data_path+'ga_cid_sbbol_inn'+'/'+'{}/'.format(ctl)\n",
    "\n",
    "    subprocess.call(['hdfs', 'dfs', '-cp', '-f', hdfs_from, hdfs_to], stdout=subprocess.PIPE, stdin=subprocess.PIPE)\n",
    "#     res= re.sub(\"\\s+\",\" \",p.communicate()[0].decode('utf-8'))\n",
    "#     print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hive.sql(\"msck repair table sbx_team_digitcamp.ga_cid_sbbol_inn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for availability of newly added partititons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hive.sql(\"select * from sbx_team_digitcamp.ga_cid_sbbol_inn where ctl_loading=11482743\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
