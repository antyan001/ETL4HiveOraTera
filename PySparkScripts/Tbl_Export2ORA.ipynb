{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!/home/ektov1-av_ca-sbrf-ru/bin/python35\n",
    "import os\n",
    "import sys\n",
    "curruser = os.environ.get('USER')\n",
    "\n",
    "sys.path.insert(0, '/opt/workspace/{user}/notebooks/support_library/'.format(user=curruser))\n",
    "sys.path.insert(0, '/opt/workspace/{user}/libs/python3.5/site-packages/'.format(user=curruser))\n",
    "sys.path.insert(0, '/opt/workspace/{user}/notebooks/labdata/lib/'.format(user=curruser))\n",
    "\n",
    "# sys.path.insert(0, '/src/')\n",
    "\n",
    "# import tendo.singleton\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig(filename='./__upd_runGASiteProd2SAS__.log',level=logging.INFO,\n",
    "#                     format='%(asctime)s %(levelname)s %(name)s %(message)s')\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "import joblib\n",
    "import json\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from time import sleep\n",
    "from itertools import islice\n",
    "from multiprocessing import Pool, Process, JoinableQueue\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from functools import partial\n",
    "import subprocess\n",
    "from threading import Thread\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from spark_connector import SparkConnector\n",
    "from sparkdb_loader import spark\n",
    "from connector import OracleDB\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf, HiveContext\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import loader as load\n",
    "# from etl import ETLORA\n",
    "\n",
    "\n",
    "# sing = tendo.singleton.SingleInstance()\n",
    "\n",
    "# os.chdir('/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/Clickstream_Analytics/AutoUpdate/')\n",
    "# os.chdir('/opt/workspace/{}/notebooks/clickstream/AutoUpdate/'.format(curruser))\n",
    "\n",
    "\n",
    "##-----------------------------------------\n",
    "conn_schema = 'sbx_team_digitcamp'\n",
    "ga_schema = 'sklod_external_google_analytics'\n",
    "tbl = 'GA_SITE_ALL_PRODS'\n",
    "out_table_name = 'GA_ALL_SCENARIOS_HIST'\n",
    "##-----------------------------------------http://pklis-chd000224.labiac.df.sbrf.ru:8100/notebooks/ISKRA_GET_LOAD/Hive2Iskra.ipynb#\n",
    "\n",
    "def show(self, n=10):\n",
    "    return self.limit(n).toPandas()\n",
    "\n",
    "pyspark.sql.dataframe.DataFrame.show = show\n",
    "\n",
    "def typed_udf(return_type):\n",
    "    '''Make a UDF decorator with the given return type'''\n",
    "\n",
    "    def _typed_udf_wrapper(func):\n",
    "        return f.udf(func,return_type)\n",
    "\n",
    "# @typed_udf(StringType())\n",
    "def match_product(product_dict_nm):\n",
    "    def get_product(product_norm, product_dict_nm):\n",
    "        if product_norm is None:\n",
    "            return None\n",
    "\n",
    "        for product_nm in product_dict_nm:\n",
    "            if product_nm in product_norm:\n",
    "                return product_dict_nm[product_nm]\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    return f.udf(lambda x: get_product(x,product_dict_nm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================== 2022-05-16 ======================================================\n",
      "# __init__ : begin\n",
      "2.4.0.cloudera2\n"
     ]
    }
   ],
   "source": [
    "sp = spark(schema=conn_schema,\n",
    "               dynamic_alloc=False,\n",
    "               numofinstances=8,\n",
    "               numofcores=8,\n",
    "               executor_memory='25g',\n",
    "               driver_memory='20g',\n",
    "               kerberos_auth=False,\n",
    "               process_label=\"SAS_OFFER_REPARTITION_\"\n",
    "               )\n",
    "\n",
    "hive = sp.sql\n",
    "print(sp.sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sdf = hive.table(\"sbx_t_team_cvm.bigartm_inf_v_1_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sdf = sdf.select([col.upper() for col in sdf.columns])\n",
    "sdf = sdf.withColumnRenamed(\"COMMENT\", \"COMMENT_KM_TXT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NAT_MASK      = \"2099-01-01\"\n",
    "NAT_YEAR_MASK = \"2099\"\n",
    "SEP=\";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return some str columns type to Numeric type due to possible wrong type infering after Oracle import\n",
      "invalid literal for int() with base 10: ''\n",
      "could not convert string to float: \n",
      "invalid literal for int() with base 10: ''\n",
      "could not convert string to float: \n",
      "invalid literal for int() with base 10: '[Скидка], Рекламная кампания от 2ГИС со скидкой. Кемеровская Область'\n",
      "could not convert string to float: '[Скидка], Рекламная кампания от 2ГИС со скидкой. Кемеровская Область'\n",
      "invalid literal for int() with base 10: '1-5N3D0I3S'\n",
      "could not convert string to float: '1-5N3D0I3S'\n",
      "invalid literal for int() with base 10: ''\n",
      "could not convert string to float: \n",
      "## Work with TimeStamp cols to find `ValueError: year is out of range`\n",
      "MODEL_ID              object\n",
      "COMMENT_SOURCE        object\n",
      "INN                   object\n",
      "COMMENT               object\n",
      "PROCESSED_DAY_D       object\n",
      "PROD_OFFER_CRM_ID     object\n",
      "TASK_CRM_ID           object\n",
      "CLUST_0              float64\n",
      "CLUST_1              float64\n",
      "CLUST_2              float64\n",
      "CLUST_3              float64\n",
      "CLUST_4              float64\n",
      "CLUST_5              float64\n",
      "CLUST_6              float64\n",
      "CLUST_7              float64\n",
      "CLUST_8              float64\n",
      "CLUST_9              float64\n",
      "CLUST_10             float64\n",
      "CLUST_11             float64\n",
      "CLUST_12             float64\n",
      "CLUST_13             float64\n",
      "CLUST_14             float64\n",
      "CLUST_15             float64\n",
      "CLUST_16             float64\n",
      "CLUST_17              object\n",
      "CLUST_18              object\n",
      "CLUST_19              object\n",
      "CLUST_20              object\n",
      "CLUST_21              object\n",
      "CLUST_22              object\n",
      "CLUST_23              object\n",
      "CLUST_24              object\n",
      "CLUST_25              object\n",
      "CLUST_26              object\n",
      "CLUST_27              object\n",
      "CLUST_28              object\n",
      "CLUST_29              object\n",
      "CLUST_30              object\n",
      "CLUST_31              object\n",
      "CLUST_32              object\n",
      "CLUST_33              object\n",
      "CLUST_34              object\n",
      "CLUST_35              object\n",
      "CLUST_36              object\n",
      "CLUST_37              object\n",
      "CLUST_38              object\n",
      "CLUST_39              object\n",
      "CLUST_40              object\n",
      "CLUST_41              object\n",
      "CLUST_42              object\n",
      "CLUST_43              object\n",
      "CLUST_44              object\n",
      "CLUST_45              object\n",
      "CLUST_46              object\n",
      "CLUST_47              object\n",
      "CLUST_48              object\n",
      "CLUST_49              object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "print(\"Return some str columns type to Numeric type due to possible wrong type infering after Oracle import\")\n",
    "\n",
    "for col, _type in sdf.dtypes:\n",
    "    if _type == 'string':\n",
    "        if col.lower() != 'inn':\n",
    "            df = sdf.select(f.col(col)).filter(~f.col(col).isNull()).limit(10).toPandas()\n",
    "            if df.shape[0] > 0:\n",
    "                val = df.loc[df[col].first_valid_index(), col]\n",
    "                try:\n",
    "                    res = int(val)\n",
    "                    sdf = sdf.withColumn(col, f.col(col).cast(IntegerType()))\n",
    "                except ValueError as ex:\n",
    "                    # 'invalid literal for int() with base 10'\n",
    "                    print(str(ex))\n",
    "                    try:\n",
    "                        res = float(val)\n",
    "                        sdf = sdf.withColumn(col, f.col(col).cast(FloatType()))\n",
    "                    except ValueError as ex:\n",
    "                        # 'could not convert string to float'\n",
    "                        print(str(ex))\n",
    "\n",
    "        sdf = sdf.fillna({col: ''})\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "print(\"## Work with TimeStamp cols to find `ValueError: year is out of range`\")\n",
    "col_dt = []\n",
    "for col, _type in sdf.dtypes:\n",
    "    if _type == 'timestamp':\n",
    "        col_dt.append(col)\n",
    "\n",
    "for col in col_dt:\n",
    "    sdf = sdf.withColumn(col,\n",
    "                         (f.regexp_replace( f.col(col).cast(StringType()),\n",
    "                                            \"^([013456789]{4})(?:\\-).*\",\n",
    "                                            NAT_YEAR_MASK)).cast(TimestampType())\n",
    "                        )\n",
    "    sdf = sdf.withColumn(col, f.coalesce(col, f.lit(pd.to_datetime(NAT_MASK))))\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "for col, _type in sdf.dtypes:\n",
    "    if _type == 'string':\n",
    "        sdf = sdf.withColumn(col, f.regexp_replace(col,'[\\{}]'.format(SEP), \"\"))\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "df = sdf.limit(1000).toPandas()\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['PROCESSED_DAY_D'] = df['PROCESSED_DAY_D'].apply(lambda x: pd.to_datetime(datetime.strftime(x, format='%Y-%m-%d'), \n",
    "                                                                     format='%Y-%m-%d')\n",
    "                                           )\n",
    "\n",
    "# df.rename(columns={\"COMMENT\":\"COMMENT_KM_TXT\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create MetaTable in Oracle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "str_ = 'CREATE TABLE {} ( '\n",
    "for column_name, column in df.iteritems():\n",
    "    print(column_name)\n",
    "    if (isinstance(column[column.first_valid_index()], str)):\n",
    "        if (df[column_name].str.len().max() >= 4000):\n",
    "            str_+=column_name.upper() + ' ' +'CLOB, '\n",
    "        else:\n",
    "            if 'INN' in column_name.upper() or 'KPP' in column_name.upper():\n",
    "                str_+=column_name.upper() + ' ' +'VARCHAR2(20), '\n",
    "            else:\n",
    "                str_+=column_name.upper() + ' ' +'VARCHAR2(800), '\n",
    "    elif column.dtype.kind == 'i':\n",
    "        str_+=column_name.upper() + ' ' +'NUMBER(20), '\n",
    "    elif column.dtype.kind == 'f':\n",
    "        if 'TARGET' in column_name.upper():\n",
    "            str_+=column_name.upper() + ' ' +'NUMBER(1), '\n",
    "        else:\n",
    "            df[column_name] = df[column_name].fillna(0.0)\n",
    "            str_+=column_name.upper() + ' ' +'FLOAT(126), '        \n",
    "    elif column.dtype.kind == 'M':\n",
    "        if 'PROCESSED_DAY_D' in column_name.upper():\n",
    "            str_+=column_name.upper() + ' ' +'DATE, '\n",
    "        else:\n",
    "            str_+=column_name.upper() + ' ' +'TIMESTAMP, '\n",
    "    elif column.dtype.kind == 'b':\n",
    "        str_+=column_name.upper() + ' ' +'BOOLEAN, '        \n",
    "    else:\n",
    "        None  \n",
    "res=str_.strip()[:-1] + ' )' \\\n",
    "''' \n",
    "PARTITION BY RANGE (PROCESSED_DAY_D) \n",
    "INTERVAL(NUMTODSINTERVAL(1,'DAY')) \n",
    "(PARTITION SYS_P1 VALUES LESS THAN (TO_DATE('2021-01-01', 'YYYY-MM-DD')),\n",
    " PARTITION SYS_P2 VALUES LESS THAN (TO_DATE('2021-01-02', 'YYYY-MM-DD'))\n",
    ")\n",
    "'''\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ld=load.Loader(init_dsn=True, encoding='cp1251',  sep=',')\n",
    "db = load.OracleDB('ISKRA_CVM', 'hud2X_3VRCe5Lw_rpcN0oCu5', ld._get_dsn('iskra4')) \n",
    "db.connect()\n",
    "curs = db.cursor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating done\n"
     ]
    }
   ],
   "source": [
    "table_name_new = 'bigartm_topicks_km'\n",
    "\n",
    "# sql = \"DROP TABLE {}\".format(table_name_new)\n",
    "# db.cursor.execute(sql)\n",
    "# db.connection.commit()\n",
    "# print('dropping done')\n",
    "\n",
    "sql = res.format(table_name_new)\n",
    "\n",
    "db.cursor.execute(sql)\n",
    "db.connection.commit()\n",
    "print('creating done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export via jDBC driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MODEL_ID VARCHAR(900), COMMENT_SOURCE VARCHAR(900), INN VARCHAR(20), COMMENT_KM_TXT VARCHAR(900), PROCESSED_DAY_D DATE, PROD_OFFER_CRM_ID VARCHAR(900), TASK_CRM_ID VARCHAR(900), CLUST_0 FLOAT, CLUST_1 FLOAT, CLUST_2 FLOAT, CLUST_3 FLOAT, CLUST_4 FLOAT, CLUST_5 FLOAT, CLUST_6 FLOAT, CLUST_7 FLOAT, CLUST_8 FLOAT, CLUST_9 FLOAT, CLUST_10 FLOAT, CLUST_11 FLOAT, CLUST_12 FLOAT, CLUST_13 FLOAT, CLUST_14 FLOAT, CLUST_15 FLOAT, CLUST_16 FLOAT, CLUST_17 FLOAT, CLUST_18 FLOAT, CLUST_19 FLOAT, CLUST_20 FLOAT, CLUST_21 FLOAT, CLUST_22 FLOAT, CLUST_23 FLOAT, CLUST_24 FLOAT, CLUST_25 FLOAT, CLUST_26 FLOAT, CLUST_27 FLOAT, CLUST_28 FLOAT, CLUST_29 FLOAT, CLUST_30 FLOAT, CLUST_31 FLOAT, CLUST_32 FLOAT, CLUST_33 FLOAT, CLUST_34 FLOAT, CLUST_35 FLOAT, CLUST_36 FLOAT, CLUST_37 FLOAT, CLUST_38 FLOAT, CLUST_39 FLOAT, CLUST_40 FLOAT, CLUST_41 FLOAT, CLUST_42 FLOAT, CLUST_43 FLOAT, CLUST_44 FLOAT, CLUST_45 FLOAT, CLUST_46 FLOAT, CLUST_47 FLOAT, CLUST_48 FLOAT, CLUST_49 FLOAT'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typesmap={}\n",
    "for column_name, column in sdf.dtypes:\n",
    "    if column == 'string':\n",
    "        if 'INN' in column_name.upper() or 'KPP' in column_name.upper():\n",
    "            typesmap[column_name] = 'VARCHAR(20)'\n",
    "        elif 'commonSegmentoUID'.upper() in column_name.upper():\n",
    "            typesmap[column_name] = 'CLOB'\n",
    "        else:\n",
    "            typesmap[column_name] = 'VARCHAR(900)'\n",
    "    elif column == 'int':\n",
    "        typesmap[column_name] = 'INTEGER'\n",
    "    elif column == 'bigint':\n",
    "        typesmap[column_name] = 'LONG'\n",
    "    elif column == 'timestamp':\n",
    "        typesmap[column_name] = 'TIMESTAMP'\n",
    "    elif column == 'date':\n",
    "        typesmap[column_name] = 'DATE'        \n",
    "    elif column == 'float' or column == 'double':\n",
    "        typesmap[column_name] = 'FLOAT'\n",
    "    else:\n",
    "        None\n",
    "        \n",
    "cols = ', '.join([col + ' ' + typesmap[col] for col in sdf.columns])\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = OracleDB('iskra4')\n",
    "mode = 'append'\n",
    "sdf \\\n",
    "    .write \\\n",
    "    .format('jdbc') \\\n",
    "    .mode(mode) \\\n",
    "    .option('url', 'jdbc:oracle:thin:@//'+db.dsn) \\\n",
    "    .option('user', db.user) \\\n",
    "    .option('password', db.password) \\\n",
    "    .option('dbtable', table_name_new) \\\n",
    "    .option('createTableColumnTypes', cols)\\\n",
    "    .option('driver', 'oracle.jdbc.driver.OracleDriver') \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check saved DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "(select /*+ parallel (4) */\n",
    "    *      \n",
    "FROM {}\n",
    "where rownum < 1000)\n",
    "'''.format(table_name_new)\n",
    "\n",
    "sdf = sp.get_oracle(OracleDB('iskra4'), query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
