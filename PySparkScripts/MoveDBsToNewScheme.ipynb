{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/home/ektov1-av_ca-sbrf-ru/bin/python35\n",
    "import os\n",
    "import sys\n",
    "curruser = os.environ.get('USER')\n",
    "\n",
    "# sys.path.insert(0, '/opt/workspace/{user}/system/support_library/'.format(user=curruser))\n",
    "# sys.path.insert(0, '/opt/workspace/{user}/libs/'.format(user=curruser))\n",
    "# sys.path.insert(0, '/opt/workspace/{user}/system/labdata/lib/'.format(user=curruser))\n",
    "\n",
    "\n",
    "sys.path.insert(0, '/opt/workspace/{user}/notebooks/support_library/'.format(user=curruser))\n",
    "sys.path.insert(0, '/opt/workspace/{user}/libs/python3.5/site-packages/'.format(user=curruser))\n",
    "sys.path.insert(0, '/opt/workspace/{user}/notebooks/labdata/lib/'.format(user=curruser))\n",
    "sys.path.insert(0, './src')\n",
    "\n",
    "#import tendo.singleton\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(filename='./__move_dbs__.log',level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(name)s %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "import joblib\n",
    "import json\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from time import sleep\n",
    "from itertools import islice\n",
    "from multiprocessing import Pool, Process, JoinableQueue\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from functools import partial\n",
    "import subprocess\n",
    "from threading import Thread\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from transliterate import translit\n",
    "\n",
    "from spark_connector import SparkConnector\n",
    "from sparkdb_loader import spark\n",
    "from connector import OracleDB\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf, HiveContext\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import loader as load\n",
    "\n",
    "# sing = tendo.singleton.SingleInstance()\n",
    "\n",
    "# os.chdir('/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/Clickstream_Analytics/AutoUpdate/')\n",
    "# os.chdir('/opt/workspace/{}/notebooks/clickstream/AutoUpdate/'.format(curruser))\n",
    "\n",
    "\n",
    "##-----------------------------------------\n",
    "conn_schema = 'sbx_team_digitcamp' #'dl_t_team_ds_kb_sme' 'dl_t_team_speech_analytics'\n",
    "new_schema = 'sbx_t_team_cvm'\n",
    "ga_schema = 'sklod_external_google_analytics'\n",
    "##-----------------------------------------\n",
    "\n",
    "\n",
    "def show(self, n=10):\n",
    "    return self.limit(n).toPandas()\n",
    "\n",
    "def typed_udf(return_type):\n",
    "    '''Make a UDF decorator with the given return type'''\n",
    "\n",
    "    def _typed_udf_wrapper(func):\n",
    "        return f.udf(func,return_type)\n",
    "\n",
    "def essense(channel: str, prod_cd: str):\n",
    "    message = \"{}: {} retargeting\".format(channel, prod_cd)\n",
    "    return message\n",
    "\n",
    "essense_udf = f.udf(essense, StringType())\n",
    "\n",
    "pyspark.sql.dataframe.DataFrame.show = show\n",
    "\n",
    "def print_and_log(message: str):\n",
    "    print(message)\n",
    "    logger.info(message)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Starting spark context. Run!\n",
      "2.4.0.cloudera2\n"
     ]
    }
   ],
   "source": [
    "print(\"### Starting spark context. Run!\")\n",
    "\n",
    "sp = spark(schema=conn_schema,\n",
    "           dynamic_alloc=False,\n",
    "           numofinstances=10, \n",
    "           numofcores=8,\n",
    "           kerberos_auth=True,\n",
    "           process_label=\"SAS_\"\n",
    "           )\n",
    "hive = sp.sql\n",
    "\n",
    "print(sp.sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CRM_PRODUCT_ID</th>\n",
       "      <th>PRODUCT_CRM_NM</th>\n",
       "      <th>CRM_ACTIVE_FLG</th>\n",
       "      <th>PRODUCT_CD_KSB</th>\n",
       "      <th>PRODUCT_CD_RGS</th>\n",
       "      <th>PRODUCT_CD_MMB</th>\n",
       "      <th>PRODUCT_CD_ASUP</th>\n",
       "      <th>PRODUCT_SHORT_NM</th>\n",
       "      <th>PRODUCT_FULL_NM</th>\n",
       "      <th>...</th>\n",
       "      <th>CHANNEL_MMB_CKR</th>\n",
       "      <th>CHANNEL_MMB_MKK</th>\n",
       "      <th>CHANNEL_DZO</th>\n",
       "      <th>AGG_FORMULA_KSB</th>\n",
       "      <th>AGG_FORMULA_RGS</th>\n",
       "      <th>CNT_DAY_DEAL</th>\n",
       "      <th>PRODUCT_SUBGROUP</th>\n",
       "      <th>PRODUCT_GROUP</th>\n",
       "      <th>PRODUCT_TYPE</th>\n",
       "      <th>PRODUCT_PRIORITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4344</td>\n",
       "      <td>1-2AZ4PW6S</td>\n",
       "      <td>2GIS-Вывеска</td>\n",
       "      <td>1</td>\n",
       "      <td>2170</td>\n",
       "      <td>2170</td>\n",
       "      <td>2GIS_VIVESKA</td>\n",
       "      <td>two_gis</td>\n",
       "      <td>2GIS-Вывеска</td>\n",
       "      <td>2GIS-Вывеска</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2GIS</td>\n",
       "      <td>Экосистема</td>\n",
       "      <td>Экосистема</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID CRM_PRODUCT_ID PRODUCT_CRM_NM  CRM_ACTIVE_FLG PRODUCT_CD_KSB  \\\n",
       "0  4344     1-2AZ4PW6S   2GIS-Вывеска               1           2170   \n",
       "\n",
       "  PRODUCT_CD_RGS PRODUCT_CD_MMB PRODUCT_CD_ASUP PRODUCT_SHORT_NM  \\\n",
       "0           2170   2GIS_VIVESKA         two_gis     2GIS-Вывеска   \n",
       "\n",
       "  PRODUCT_FULL_NM  ... CHANNEL_MMB_CKR  CHANNEL_MMB_MKK  CHANNEL_DZO  \\\n",
       "0    2GIS-Вывеска  ...               1                0            0   \n",
       "\n",
       "   AGG_FORMULA_KSB  AGG_FORMULA_RGS  CNT_DAY_DEAL  PRODUCT_SUBGROUP  \\\n",
       "0                0                0          30.0              2GIS   \n",
       "\n",
       "   PRODUCT_GROUP  PRODUCT_TYPE  PRODUCT_PRIORITY  \n",
       "0     Экосистема    Экосистема                 0  \n",
       "\n",
       "[1 rows x 27 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hive.sql(''' SELECT * FROM sbx_team_digitcamp.ma_dict_v_product_dict\n",
    "             where crm_product_id = '1-2AZ4PW6S' ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table_name = 'telemarketing_cell2inn' #'MA_CMDM_MA_PRODUCT_OFFER'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = hive.sql(\"describe extended {}.{}\".format(conn_schema,table_name)).collect()\n",
    "len([item.asDict() for item in res if item.asDict()['col_name'] =='# Partition Information']) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hive.sql('''ALTER TABLE {}.{} \n",
    "            DROP IF EXISTS PARTITION(host_prod_id='__HIVE_DEFAULT_PARTITION__')'''.format(conn_schema,table_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-04-01'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts = hive.sql(\"show partitions {}.{}\".format(conn_schema,table_name)).collect()\n",
    "max_part = sorted(parts,reverse=True)[0]['partition']\n",
    "extract_date=re.compile(\"\\d{4}\\-\\d{2}\\-\\d{2}\")\n",
    "ext = extract_date.search(max_part)\n",
    "try:\n",
    "    max_trunc_dt = ext.group(0)\n",
    "except:\n",
    "    max_trunc_dt = None\n",
    "max_trunc_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(creation_dttm)=datetime.datetime(2021, 4, 19, 6, 51, 14))]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql = '''select max(creation_dttm) from {}.{} where creation_month = '{}' '''.format(conn_schema,table_name,max_trunc_dt)\n",
    "max_dt = hive.sql(sql).collect()\n",
    "max_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list of tables without partitioned column\n",
    "dbslst =\\\n",
    "[\n",
    "#     'MA_CMDM_MA_DEAL',\n",
    "#     'GA_MA_CUSTOMER_SBBOL',\n",
    "#     'GA_MA_USER_PROFILE_SBBOL',\n",
    "#     'MA_MMB_OFFER_NONTOP' ,\n",
    "#     'OFFER_PRIORITY',\n",
    "#     'MA_MMB_PRODUCT_DICT',\n",
    "#     'UNIFIED_CUSTOMER' ,\n",
    "#     'MA_CMDM_MA_AGREEMENT',\n",
    "#     'SME_INT_PIPELINE_BASE',\n",
    "#     'SME_CDM_V_RESPONSE',\n",
    "#     'SME_EMAIL_TO_SENDSAY',\n",
    "#     'SME_INTEGRATION_SMS_QUERY',\n",
    "#     'SME_CDM_CI_CELL_PACKAGE',\n",
    "#      'mw_atb_segmen_prod_group'    \n",
    "#     'CALL_TASK_TO_SENDSAY',\n",
    "#     'RESP_INN_KPP_CAMP',\n",
    "#     'RESP_SMS_INN_KPP_CAMP',    \n",
    "#     'RESP_INN_KPP_PROBA',\n",
    "#     'RESP_INN_KPP_CAMP_PROBA',\n",
    "#     'RESP_INN_KPP_CAMP_EMAIL_PROBA',\n",
    "#     'CALLS_UUP_DETAIL',\n",
    "#     'SAS_A1_21_TE_COL_N',\n",
    "#     'SAS_Y1_01_KORSAKOV_PRIORITY',\n",
    "#     'AIST_INTERACTION_HISTORY',\n",
    "#     'MA_TASK'\n",
    "#     'FV_CKR_UUP_PP',\n",
    "#     'FV_CKR_UUP_VAL'\n",
    "#       'MA_MMB_OFFER_NONTOP',\n",
    "#       'OFFER_PRIORITY'\n",
    "      'INTERNAL_CLICKSTREAM_SITE_CORP',\n",
    "      'INTERNAL_CLICKSTREAM_INN_COOKIE',\n",
    "      'INTERNAL_PANHASH_COOKIES',\n",
    "      'ma_product_dict',\n",
    "      'ma_stuff',\n",
    "      'ma_mmb_offer_nontop',\n",
    "      'offer_priority'\n",
    "    \n",
    "]\n",
    "\n",
    "# list of partitioned tables\n",
    "partdbs = \\\n",
    "[    \n",
    "#     ('GA_SITE_ALL_PRODS', 'ctl_loading', 'bigint')\n",
    "#     ('GA_CID_SBBOL_INN', 'ctl_loading', 'bigint'),\n",
    "#     ('GA_ALL_SCENARIOS_HIST', 'ctl_loading', 'bigint'),\n",
    "#     ('RESP_SMS_INN_KPP_CAMP','last_day', 'string'),\n",
    "#     ('RESP_INN_KPP_CAMP', 'LAST_DAY', 'string')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replication of simple nonpartitioned tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### start replication of table: INTERNAL_CLICKSTREAM_SITE_CORP\n",
      "### table: INTERNAL_CLICKSTREAM_SITE_CORP has been succesfully replicated\n",
      "### start replication of table: INTERNAL_CLICKSTREAM_INN_COOKIE\n",
      "### table: INTERNAL_CLICKSTREAM_INN_COOKIE has been succesfully replicated\n",
      "### start replication of table: INTERNAL_PANHASH_COOKIES\n",
      "### table: INTERNAL_PANHASH_COOKIES has been succesfully replicated\n",
      "### start replication of table: ma_product_dict\n",
      "### table: ma_product_dict has been succesfully replicated\n",
      "### start replication of table: ma_stuff\n",
      "### table: ma_stuff has been succesfully replicated\n",
      "### start replication of table: ma_mmb_offer_nontop\n",
      "### table: ma_mmb_offer_nontop has been succesfully replicated\n",
      "### start replication of table: offer_priority\n",
      "### table: offer_priority has been succesfully replicated\n"
     ]
    }
   ],
   "source": [
    "for tbl in dbslst:\n",
    "    print_and_log(\"### start replication of table: {}\".format(tbl))\n",
    "    hive.sql(\"drop table if exists {new_schema}.{tbl}\".format(new_schema=new_schema, tbl=tbl))\n",
    "    query = \"create table {new_schema}.{tbl} select * from {schema}.{tbl}\".format(schema=conn_schema,\n",
    "                                                                                  new_schema=new_schema,\n",
    "                                                                                  tbl=tbl)\n",
    "    hive.sql(query)\n",
    "    subprocess.call(['hdfs', 'dfs', '-chmod', '-R', '777', \"hdfs://clsklsbx/user/team/team_digitcamp/hive/{}\".format(tbl.lower())])\n",
    "    print_and_log(\"### table: {} has been succesfully replicated\".format(tbl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replication of partitioned tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn_schema = 'sbx_team_digitcamp' #'dl_t_team_ds_kb_sme' 'dl_t_team_speech_analytics'\n",
    "new_schema = 'sbx_t_team_cvm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hive.setConf(\"hive.exec.dynamic.partition\",\"true\")\n",
    "hive.setConf(\"hive.exec.dynamic.partition.mode\",\"nonstrict\")\n",
    "hive.setConf(\"hive.enforce.bucketing\",\"false\")\n",
    "hive.setConf(\"hive.enforce.sorting\",\"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "part_cols  =  [('interaction_end_month',),\n",
    "               ('string',)\n",
    "              ]\n",
    "bucket_col = [\"end_plan_dt\", \"end_fact_dt\", \"inn\"]\n",
    "bucket_num = 125\n",
    "\n",
    "table_name = 'AIST_INTERACTION_HISTORY'\n",
    "part_tbl = \"tmp_\"+table_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sdf = hive.sql(\"select * from {}.{}\".format(conn_schema,table_name))\n",
    "# sdf = sp.get_oracle(OracleDB('iskra4'), '''( select /*+ parallel (12) */ * from MA_CMDM_MA_DEAL \n",
    "#                                              where create_dt <= to_timestamp('2020-08-01 00:00:00', 'yyyy-mm-dd hh24:mi:ss')\n",
    "#                                             )''')\n",
    "sdf = sdf.withColumn('interaction_end_month',f.trunc(f.to_date('interaction_end_date'),'MONTH').cast(StringType()))\n",
    "# strdate = datetime.strftime(datetime.now(), format='%Y.%d.%m')\n",
    "# currdate = datetime.strptime(strdate,'%Y.%d.%m')\n",
    "# sdf = sdf.withColumn('load_dttm',f.lit(currdate).cast(TimestampType()))\n",
    "# sdf.registerTempTable(\"tmp_insight\")\n",
    "# sdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Repartitioned table based on the origin table in the same scheme (with '_NEW' suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_and_log(\"### start replication of table: {}\".format(table_name))\n",
    "\n",
    "hive.sql(\"drop table if exists {schema}.{tbl} purge\".format(schema=conn_schema, tbl=table_name+'_NEW'))\n",
    "#     sdf = hive.sql(\"select * from {}.{}\".format(conn_schema,tbl))\n",
    "insert = ', '.join([\"{} {}\".format(col, _type) for col, _type in sdf.dtypes if col.lower() not in part_cols[0]])\n",
    "part_tupl_lst = ', '.join([\"{} {}\".format(col, _type) for col, _type in list(zip(part_cols[0],part_cols[1]))])\n",
    "part_col_lst  = ', '.join([col for col in part_cols[0]])\n",
    "bucket_cols = ', '.join([col for col in bucket_col])   \n",
    "\n",
    "hive.sql('''create table {schema}.{tbl} (\n",
    "                                         {fields}\n",
    "                                            )\n",
    "             PARTITIONED BY ({part_col_lst})\n",
    "             CLUSTERED BY ({bucket_cols}) INTO {bucket_num} BUCKETS STORED AS PARQUET\n",
    "         '''.format(schema=conn_schema,\n",
    "                    tbl=table_name+'_NEW',\n",
    "                    fields=insert,\n",
    "                    part_col_lst=part_tupl_lst,\n",
    "                    bucket_num=bucket_num,\n",
    "                    bucket_cols=bucket_cols)\n",
    "        )\n",
    "    \n",
    "colsinhive = hive.sql(\"select * from {}.{}\".format(conn_schema,table_name+'_NEW')).columns    \n",
    "sdf = sdf.select(*colsinhive)\n",
    "sdf.registerTempTable(part_tbl)\n",
    "\n",
    "hive.sql(\"\"\"\n",
    "insert into table {schema}.{tbl}\n",
    "partition({part_col})\n",
    "select * from {tmp_tbl}\n",
    "cluster by ({bucket_cols})\n",
    "\"\"\".format(schema=conn_schema,\n",
    "           tbl=table_name+'_NEW',\n",
    "           tmp_tbl=part_tbl,\n",
    "           part_col=part_col_lst,\n",
    "           bucket_cols=bucket_cols)\n",
    "        )    \n",
    "\n",
    "subprocess.call(['hdfs', 'dfs', '-chmod', '-R', '777', \"hdfs://clsklsbx/user/team/team_digitcamp/hive/{}\".format(table_name+'_NEW'.lower())])\n",
    "\n",
    "print_and_log(\"### table: {} has been succesfully replicated\".format(tbl))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Recplication between different schemes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### start replication of table: MA_CMDM_MA_TASK\n",
      "### table: MA_CMDM_MA_TASK has been succesfully replicated\n"
     ]
    }
   ],
   "source": [
    "print_and_log(\"### start replication of table: {}\".format(table_name))\n",
    "\n",
    "hive.sql(\"drop table if exists {new_schema}.{tbl} purge\".format(new_schema=new_schema, tbl=table_name))\n",
    "#     sdf = hive.sql(\"select * from {}.{}\".format(conn_schema,tbl))\n",
    "insert = ', '.join([\"{} {}\".format(col, _type) for col, _type in sdf.dtypes if col.lower() not in part_cols[0]])\n",
    "part_tupl_lst = ', '.join([\"{} {}\".format(col, _type) for col, _type in list(zip(part_cols[0],part_cols[1]))])\n",
    "part_col_lst  = ', '.join([col for col in part_cols[0]])\n",
    "bucket_cols = ', '.join([col for col in bucket_col])\n",
    "\n",
    "#     hive.sql('''create table {schema}.{tbl} (\n",
    "#                                              {fields}\n",
    "#                                                 )\n",
    "#                  PARTITIONED BY ({part_col} {col_type})\n",
    "#              '''.format(schema=new_schema,\n",
    "#                         tbl=tbl, \n",
    "#                         fields=insert, \n",
    "#                         part_col=part_col,\n",
    "#                         col_type=_type)\n",
    "#             )    \n",
    "\n",
    "hive.sql('''create table {schema}.{tbl} (\n",
    "                                         {fields}\n",
    "                                            )\n",
    "             PARTITIONED BY ({part_col_lst})\n",
    "             CLUSTERED BY ({bucket_cols}) INTO {bucket_num} BUCKETS STORED AS PARQUET\n",
    "         '''.format(schema=new_schema,\n",
    "                    tbl=table_name,\n",
    "                    fields=insert,\n",
    "                    part_col_lst=part_tupl_lst,\n",
    "                    bucket_num=bucket_num,\n",
    "                    bucket_cols=bucket_cols)\n",
    "        )\n",
    "    \n",
    "colsinhive = hive.sql(\"select * from {}.{}\".format(new_schema,table_name)).columns    \n",
    "sdf = sdf.select(*colsinhive)\n",
    "sdf.registerTempTable(part_tbl)\n",
    "\n",
    "hive.sql(\"\"\"\n",
    "insert into table {schema}.{tbl}\n",
    "partition({part_col})\n",
    "select * from {tmp_tbl}\n",
    "cluster by ({bucket_cols})\n",
    "\"\"\".format(schema=new_schema,\n",
    "           tbl=table_name,\n",
    "           tmp_tbl=part_tbl,\n",
    "           part_col=part_col_lst,\n",
    "           bucket_cols=bucket_cols)\n",
    "        )    \n",
    "\n",
    "#     hive.sql(\"\"\"\n",
    "#     insert into table {new_schema}.{tbl}\n",
    "#     partition({part_col}) \n",
    "#     select * from {schema}.{tbl}\n",
    "#     cluster by ({bucket_cols}) \n",
    "#     \"\"\".format(schema=conn_schema,\n",
    "#                new_schema=new_schema, \n",
    "#                tbl=tbl,\n",
    "#                part_col=part_col,\n",
    "#                bucket_cols=bucket_cols)\n",
    "#             )\n",
    "\n",
    "#     hive.sql(\"\"\"\n",
    "#     insert into table {new_schema}.{tbl}\n",
    "#     partition({part_col}) \n",
    "#     select * from {tmp_tbl}\n",
    "#     distribute by {part_col}\n",
    "#     \"\"\".format(new_schema=new_schema, \n",
    "#                tbl=tbl,\n",
    "#                tmp_tbl=\"tmp_insight\",\n",
    "#                part_col=part_col)\n",
    "#             )\n",
    "\n",
    "subprocess.call(['hdfs', 'dfs', '-chmod', '-R', '777', \"hdfs://clsklsbx/user/team/team_cvm/hive/{}\".format(tbl.lower())])\n",
    "\n",
    "print_and_log(\"### table: {} has been succesfully replicated\".format(tbl))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
