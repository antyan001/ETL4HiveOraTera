{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "curruser = os.environ.get('USER')\n",
    "\n",
    "_labdata = os.environ.get(\"LABDATA_PYSPARK\")\n",
    "sys.path.insert(0, _labdata)\n",
    "os.chdir(_labdata)\n",
    "\n",
    "if curruser in os.listdir(\"/opt/workspace/\"):\n",
    "    sys.path.insert(0, '/opt/workspace/{user}/notebooks/support_library/'.format(user=curruser))\n",
    "    sys.path.insert(0, '/opt/workspace/{user}/libs/python3.5/site-packages/'.format(user=curruser))\n",
    "    # sys.path.insert(0, '/opt/workspace/{user}/notebooks/labdata_v1.2/lib/'.format(user=curruser))\n",
    "else:\n",
    "    sys.path.insert(0, '/home/{}/notebooks/support_library/'.format(curruser))\n",
    "    sys.path.insert(0, '/home/{}/python35-libs/lib/python3.5/site-packages/'.format(curruser))\n",
    "    # sys.path.insert(0, '/home/{}/notebooks/labdata/lib/'.format(curruser))\n",
    "\n",
    "#import tendo.singleton\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import joblib\n",
    "import json\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from time import sleep\n",
    "from itertools import islice\n",
    "from multiprocessing import Pool, Process, JoinableQueue\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from functools import partial\n",
    "import subprocess\n",
    "from threading import Thread\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from transliterate import translit\n",
    "\n",
    "from lib.spark_connector import SparkConnector\n",
    "from lib.sparkdb_loader import *\n",
    "from lib.connector import OracleDB\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf, HiveContext\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import loader as load\n",
    "from collections import ChainMap\n",
    "\n",
    "from lib.config import *\n",
    "from lib.tools import *\n",
    "\n",
    "# sing = tendo.singleton.SingleInstance()\n",
    "\n",
    "# os.chdir('/opt/workspace/ektov/notebooks/Clickstream_Analytics/AutoUpdate/')\n",
    "# os.chdir('/opt/workspace/{}/notebooks/clickstream/AutoUpdate/'.format(curruser))\n",
    "\n",
    "def show(self, n=10):\n",
    "    return self.limit(n).toPandas()\n",
    "\n",
    "def typed_udf(return_type):\n",
    "    '''Make a UDF decorator with the given return type'''\n",
    "\n",
    "    def _typed_udf_wrapper(func):\n",
    "        return f.udf(func,return_type)\n",
    "\n",
    "    return _typed_udf_wrapper\n",
    "\n",
    "pyspark.sql.dataframe.DataFrame.show = show\n",
    "\n",
    "def print_and_log(message: str):\n",
    "    print(message)\n",
    "    logger.info(message)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate New Kerberos Ticket from PreSaved User Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Password for ektov1-av_ca-sbrf-ru@DF.SBRF.RU: \r\n"
     ]
    }
   ],
   "source": [
    "!PASS=$(cat /home/$(whoami)/pass/userpswrd | sed 's/\\r//g'); kdestroy && echo $PASS | kinit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================== 2022-01-29 ======================================================\n",
      "# __init__ : begin\n",
      "2.4.0.cloudera2\n"
     ]
    }
   ],
   "source": [
    "sp = spark(schema=CONN_SCHEMA,\n",
    "               dynamic_alloc=False,\n",
    "               numofinstances=5,\n",
    "               numofcores=8,\n",
    "               executor_memory='15g',\n",
    "               driver_memory='15g',\n",
    "               kerberos_auth=False,\n",
    "               process_label=\"_CHECK_HDFS_\"\n",
    "               )\n",
    "\n",
    "hive = sp.sql\n",
    "print(sp.sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CONN_SCHEMA = 'sbx_t_team_cvm' #'sbx_team_digitcamp' #'sbx_t_team_cvm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hive.setConf(\"hive.exec.dynamic.partition\",\"true\")\n",
    "hive.setConf(\"hive.exec.dynamic.partition.mode\",\"nonstrict\")\n",
    "hive.setConf(\"hive.enforce.bucketing\",\"false\")\n",
    "hive.setConf(\"hive.enforce.sorting\",\"false\")\n",
    "# hive.setConf(\"hive.exec.stagingdir\", \"/tmp/{}/\".format(curruser))\n",
    "# hive.setConf(\"hive.exec.scratchdir\", \"/tmp/{}/\".format(curruser))\n",
    "hive.setConf(\"hive.load.dynamic.partitions.thread\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following command generates output log file after running dfs -du command follwing with human-like sorting by size columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -du -s -h hdfs://clsklsbx/user/team/team_cvm/hive/* | awk '{print $1$2, $3$4, $5}' | sort -h -r -k2 >> ~/hdfs_consump_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Generate statistics using `describe extended` command repetitively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def table_size(hdfs_path):\n",
    "    try:\n",
    "        p = subprocess.Popen(['hdfs', 'dfs', '-du', '-s', hdfs_path], stdout=subprocess.PIPE, stdin=subprocess.PIPE)\n",
    "        res= re.sub(\"\\s+\",\" \",p.communicate()[0].decode('utf-8').rsplit(\" \",1)[0]).strip().split(\" \")\n",
    "        size, size_fr = [\"{:10.4f} Gb\".format(ele) for ele in np.array(list(map(float, res)))/1.0e9]\n",
    "    except ValueError:\n",
    "        size, size_fr = [None]*2\n",
    "    return size, size_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hdfs_dct={'OWNER':[],\n",
    "          'HDFS_PATH':[],\n",
    "          'SIZE':[],\n",
    "          'SIZE_FR':[]}\n",
    "\n",
    "dbs_row = hive.sql(\"show tables in {}\".format(CONN_SCHEMA)).collect()\n",
    "dbs_lst = [item['tableName'] for item in dbs_row]\n",
    "\n",
    "for dbs_name in dbs_lst:\n",
    "    try:\n",
    "        descr = hive.sql(\"describe formatted {}.{}\".format(CONN_SCHEMA, dbs_name)).collect()\n",
    "        descr_info = [item.asDict() for item in descr if item.asDict()['col_name'] =='Location']\n",
    "        owner_info = [item.asDict() for item in descr if item.asDict()['col_name'] =='Owner']\n",
    "        hdfs_path  = dict(ChainMap(*descr_info)).get('data_type',None)\n",
    "        owner      = dict(ChainMap(*owner_info)).get('data_type',None).split('@')[0]\n",
    "        if hdfs_path is not None:\n",
    "            hdfs_path_to_tbl = os.path.join(hdfs_path.rsplit('/',1)[0], dbs_name)\n",
    "            print(hdfs_path_to_tbl, owner)\n",
    "\n",
    "            size, size_fr = table_size(hdfs_path_to_tbl)\n",
    "            print(\"Table: {} # HDFS Consumed Space --> {}\".format(dbs_name, size))\n",
    "            hdfs_dct['OWNER'].append(owner)\n",
    "            hdfs_dct['HDFS_PATH'].append(hdfs_path_to_tbl)\n",
    "            hdfs_dct['SIZE'].extend([size.strip() if size is not None else None])\n",
    "            hdfs_dct['SIZE_FR'].extend([size_fr.strip() if size_fr is not None else None])\n",
    "            \n",
    "    except AnalysisException as ex:\n",
    "        print(str(ex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(hdfs_dct)\n",
    "new = df.assign(f = lambda x: x['SIZE_FR'].apply(lambda y: float(y.split('Gb')[0]) if y is not None else y))\\\n",
    "        .sort_values(by='f', ascending=False).drop('f',axis=1).reset_index(drop=True)\n",
    "    \n",
    "new.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['hdfs://clsklsbx/user/team/team_cvm/hive/txn_2020_12',\n",
       "        'tumanov1-ga_ca-sbrf-ru', '1462.8132 Gb', '2925.6264 Gb'],\n",
       "       ['hdfs://clsklsbx/user/team/team_cvm/hive/mon_ai_fl_embedding_v3',\n",
       "        'zhernokleev-ga_ca-sbrf-ru', '1035.6479 Gb', '2071.2959 Gb'],\n",
       "       ['hdfs://clsklsbx/user/team/team_cvm/hive/clients_products_extract',\n",
       "        'kovaleva4-oa_ca-sbrf-ru', '507.7636 Gb', '1024.8576 Gb'],\n",
       "       ['hdfs://clsklsbx/user/team/team_cvm/hive/ok_smz_dataset',\n",
       "        'kovaleva4-oa_ca-sbrf-ru', '276.9249 Gb', '553.8499 Gb'],\n",
       "       ['hdfs://clsklsbx/user/team/team_cvm/hive/mmb_offer_proc_coe',\n",
       "        'ektov1-av_ca-sbrf-ru', '165.4934 Gb', '330.9868 Gb'],\n",
       "       ['hdfs://clsklsbx/user/team/team_cvm/hive/ma_mmb_offer_nontop',\n",
       "        'ektov1-av_ca-sbrf-ru', '164.1438 Gb', '328.2877 Gb'],\n",
       "       ['hdfs://clsklsbx/user/team/team_cvm/hive/churn_seasonal_features_v2_1_pilot_19_11',\n",
       "        'geraskin1-iv_ca-sbrf-ru', '86.4890 Gb', '259.4670 Gb']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new.iloc[0:7].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new.to_excel(\"~/hdfs_team_cvm_disk_usage.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make list of tables for purging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## tbls =[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop tables recursively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for table_name in tbls:\n",
    "    hive.sql(\"drop table if exists {schema}.{tbl} purge\".format(schema=CONN_SCHEMA, tbl=table_name))\n",
    "    subprocess.call(['hdfs', 'dfs', '-rm', '-R', '-r', '-skipTrash', \n",
    "                     \"hdfs://clsklsbx/user/team/team_cvm/hive/{}\".format(table_name)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate HDFS disk consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17083.5\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -du hdfs://clsklsbx/user/team/team_cvm/hive/ | awk '{s+=$2}END{print s/1000000000}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
