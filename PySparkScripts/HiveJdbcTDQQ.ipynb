{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import logging\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "curruser = os.environ.get('USER')\n",
    "\n",
    "# sys.path.insert(0, '/opt/workspace/{user}/system/support_library/'.format(user=curruser))\n",
    "# sys.path.insert(0, '/opt/workspace/{user}/libs/'.format(user=curruser))\n",
    "# sys.path.insert(0, '/opt/workspace/{user}/system/labdata/lib/'.format(user=curruser))\n",
    "\n",
    "# sys.path.insert(0, '/opt/workspace/{}/notebooks/Clickstream_Analytics/scr'.format(curruser))\n",
    "sys.path.insert(0, '/opt/workspace/{user}/notebooks/support_library/'.format(user=curruser)) \n",
    "sys.path.insert(0, '/opt/workspace/{user}/libs/python3.5/site-packages/'.format(user=curruser))\n",
    "sys.path.insert(0, '/opt/workspace/{user}/notebooks/labdata/lib/'.format(user=curruser))\n",
    "\n",
    "# sys.path.insert(0, '/home/{}/notebooks/support_library/'.format(curruser)) \n",
    "# sys.path.insert(0, '/home/{}/python35-libs/lib/python3.5/site-packages/'.format(curruser))\n",
    "# sys.path.insert(0, '/home/{}/notebooks/labdata/lib/'.format(curruser))\n",
    "\n",
    "# from tqdm import tqdm\n",
    "# from tqdm._tqdm_notebook import tqdm_notebook\n",
    "# tqdm_notebook.pandas()\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re \n",
    "import joblib\n",
    "import subprocess\n",
    "\n",
    "# pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "# pd.set_option('display.max_colwidth', -1)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "\n",
    "from spark_connector import SparkConnector\n",
    "from sparkdb_loader import spark, log\n",
    "# from spark_helper import SparkHelper\n",
    "from connector import OracleDB\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf, HiveContext, SQLContext\n",
    "from pyspark.sql.functions import udf \n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# import loader as load\n",
    "\n",
    "# from corpora_process import utils\n",
    "import time\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# src_path = os.getcwd() + '/../../src/'\n",
    "# sys.path.insert(0, src_path) \n",
    "\n",
    "# from urlprocessing import ParseURL\n",
    "# from collections import OrderedDict\n",
    "# from CRMDictProds import *\n",
    "from datetime import datetime\n",
    "\n",
    "#----------------------------------------------------------------------------#\n",
    "#----------------------------------------------------------------------------#\n",
    "\n",
    "# logging.basicConfig(filename='./__upd_NRT_CLICKSTREAM__.log',\n",
    "#                     level=logging.INFO,\n",
    "#                     format='%(asctime)s %(levelname)s %(name)s %(message)s')\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "#----------------------------------------------------------------------------#\n",
    "#----------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show(self, n=10):\n",
    "    return self.limit(n).toPandas()\n",
    "\n",
    "pyspark.sql.dataframe.DataFrame.show = show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/cloudera/parcels/PYENV.ZNO0059623792/usr/lib/oracle/12.2/client64/lib/ojdbc8.jar:/home/ektov1-av_ca-sbrf-ru/notebooks/drivers/sqoop-connector-teradata-1.7c5.jar:/home/ektov1-av_ca-sbrf-ru/notebooks/drivers/tdgssconfig.jar:/home/ektov1-av_ca-sbrf-ru/notebooks/drivers/terajdbc4.jar'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['SPARK_CLASSPATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0.cloudera2\n",
      "CPU times: user 32.5 ms, sys: 10.9 ms, total: 43.4 ms\n",
      "Wall time: 15.3 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "====================================================== 2022-04-17 ======================================================\n",
      "# __init__ : begin\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sp = spark(schema='sklod_external_google_analytics',\n",
    "           queue = 'ektov1-av_ca-sbrf-ru',\n",
    "           process_label=\"NRT_SBBOL_\",\n",
    "           numofinstances=8, \n",
    "           numofcores=8,\n",
    "           executor_memory='25g', \n",
    "           driver_memory='25g',\n",
    "           dynamic_alloc=False,\n",
    "           kerberos_auth=True,\n",
    "           need_oracle=True\n",
    "          ) \n",
    "print(sp.sc.version)\n",
    "hive = sp.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sp.sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spark.app.id': 'application_1645879035230_1950',\n",
       " 'spark.app.name': 'NRT_SBBOL__sklod_external_google_analytics',\n",
       " 'spark.authenticate': 'false',\n",
       " 'spark.blacklist.enabled': 'true',\n",
       " 'spark.blacklist.task.maxTaskAttemptsPerExecutor': '13',\n",
       " 'spark.blacklist.task.maxTaskAttemptsPerNode': '13',\n",
       " 'spark.default.parallelism': '80',\n",
       " 'spark.driver.allowMultipleContexts': 'true',\n",
       " 'spark.driver.appUIAddress': 'http://pklis-chd002207.labiac.df.sbrf.ru:4448',\n",
       " 'spark.driver.extraClassPath': '/opt/cloudera/parcels/PYENV.ZNO0059623792/usr/lib/oracle/12.2/client64/lib/ojdbc8.jar:/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hive-jdbc-1.1.0-cdh5.16.2-standalone.jar:/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hive-exec-1.1.0-cdh5.16.2.jar:/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hive-service-1.1.0-cdh5.16.2.jar:/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hive-metastore-1.1.0-cdh5.16.2.jar:/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/commons-configuration-1.6.jar:/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/commons-logging-1.1.3.jar:/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/libthrift-0.9.0.jar:/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/httpclient-4.2.5.jar:/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/httpcore-4.2.5.jar:/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hadoop-auth-2.6.0-cdh5.16.2.jar:/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hadoop-core-2.6.0-mr1-cdh5.16.2.jar:/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hadoop-common-2.6.0-cdh5.16.2.jar',\n",
       " 'spark.driver.extraJavaOptions': '-Djavax.security.auth.useSubjectCredsOnly=false',\n",
       " 'spark.driver.extraLibraryPath': '/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/support_library/',\n",
       " 'spark.driver.host': 'pklis-chd002207.labiac.df.sbrf.ru',\n",
       " 'spark.driver.maxResultSize': '90g',\n",
       " 'spark.driver.memory': '25g',\n",
       " 'spark.driver.port': '37351',\n",
       " 'spark.dynamicAllocation.enabled': 'false',\n",
       " 'spark.dynamicAllocation.executorIdleTimeout': '60',\n",
       " 'spark.dynamicAllocation.minExecutors': '0',\n",
       " 'spark.dynamicAllocation.schedulerBacklogTimeout': '1',\n",
       " 'spark.eventLog.dir': 'hdfs://nsld3/user/spark/spark2ApplicationHistory',\n",
       " 'spark.eventLog.enabled': 'true',\n",
       " 'spark.executor.cores': '8',\n",
       " 'spark.executor.extraClassPath': '/opt/cloudera/parcels/PYENV.ZNO0059623792/usr/lib/oracle/12.2/client64/lib/ojdbc8.jar:/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hive-jdbc-1.1.0-cdh5.16.2-standalone.jar:/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hive-exec-1.1.0-cdh5.16.2.jar:/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hive-service-1.1.0-cdh5.16.2.jar:/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hive-metastore-1.1.0-cdh5.16.2.jar:/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/commons-configuration-1.6.jar:/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/commons-logging-1.1.3.jar:/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/libthrift-0.9.0.jar:/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/httpclient-4.2.5.jar:/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/httpcore-4.2.5.jar:/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hadoop-auth-2.6.0-cdh5.16.2.jar:/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hadoop-core-2.6.0-mr1-cdh5.16.2.jar:/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hadoop-common-2.6.0-cdh5.16.2.jar',\n",
       " 'spark.executor.extraJavaOptions': '-Djavax.security.auth.useSubjectCredsOnly=false',\n",
       " 'spark.executor.extraLibraryPath': '/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/support_library/',\n",
       " 'spark.executor.id': 'driver',\n",
       " 'spark.executor.instances': '8',\n",
       " 'spark.executor.memory': '25g',\n",
       " 'spark.executorEnv.PYTHONPATH': '/opt/workspace/ektov1-av_ca-sbrf-ru/libs:/opt/workspace/ektov1-av_ca-sbrf-ru/libs/python3.5/site-packages:/opt/workspace/ektov1-av_ca-sbrf-ru/libs/python3.6/site-packages:/home/ektov1-av_ca-sbrf-ru/python35-libs/lib/python3.5/site-packages:/home/ektov1-av_ca-sbrf-ru/.local/lib/python3.5/site-packages<CPS>/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/lib/py4j-0.10.7-src.zip<CPS>/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/lib/pyspark.zip',\n",
       " 'spark.extraListeners': 'com.cloudera.spark.lineage.NavigatorAppListener',\n",
       " 'spark.hadoop.dfs.replication': '2',\n",
       " 'spark.hadoop.hive.exec.max.dynamic.partitions': '50000',\n",
       " 'spark.hadoop.hive.exec.max.dynamic.partitions.pernode': '10000',\n",
       " 'spark.hadoop.hive.metastore.uris': 'thrift://pklis-chd002193.labiac.df.sbrf.ru:48869',\n",
       " 'spark.io.encryption.enabled': 'false',\n",
       " 'spark.jars': '/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/terajdbc4.jar,/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/tdgssconfig.jar,/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hive-jdbc-1.1.0-cdh5.16.2-standalone.jar,/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hive-exec-1.1.0-cdh5.16.2.jar,/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hive-service-1.1.0-cdh5.16.2.jar,/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hive-metastore-1.1.0-cdh5.16.2.jar,/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/commons-configuration-1.6.jar,/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/commons-logging-1.1.3.jar,/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/libthrift-0.9.0.jar,/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/httpclient-4.2.5.jar,/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/httpcore-4.2.5.jar,/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hadoop-auth-2.6.0-cdh5.16.2.jar,/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hadoop-core-2.6.0-mr1-cdh5.16.2.jar,/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hadoop-common-2.6.0-cdh5.16.2.jar',\n",
       " 'spark.kryoserializer.buffer.max': '1024mb',\n",
       " 'spark.lineage.enabled': 'true',\n",
       " 'spark.lineage.log.dir': '/var/log/spark2/lineage',\n",
       " 'spark.local.dir': '/home/ektov1-av_ca-sbrf-ru/tmp/',\n",
       " 'spark.master': 'yarn',\n",
       " 'spark.network.crypto.enabled': 'false',\n",
       " 'spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS': 'pklis-chd002180.labiac.df.sbrf.ru',\n",
       " 'spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES': 'https://pklis-chd002180.labiac.df.sbrf.ru:8090/proxy/application_1645879035230_1950',\n",
       " 'spark.port.maxRetries': '550',\n",
       " 'spark.rdd.compress': 'True',\n",
       " 'spark.repl.local.jars': 'file:///opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/terajdbc4.jar,file:///opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/tdgssconfig.jar,file:///opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hive-jdbc-1.1.0-cdh5.16.2-standalone.jar,file:///opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hive-exec-1.1.0-cdh5.16.2.jar,file:///opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hive-service-1.1.0-cdh5.16.2.jar,file:///opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hive-metastore-1.1.0-cdh5.16.2.jar,file:///opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/commons-configuration-1.6.jar,file:///opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/commons-logging-1.1.3.jar,file:///opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/libthrift-0.9.0.jar,file:///opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/httpclient-4.2.5.jar,file:///opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/httpcore-4.2.5.jar,file:///opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hadoop-auth-2.6.0-cdh5.16.2.jar,file:///opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hadoop-core-2.6.0-mr1-cdh5.16.2.jar,file:///opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hadoop-common-2.6.0-cdh5.16.2.jar',\n",
       " 'spark.serializer': 'org.apache.spark.serializer.KryoSerializer',\n",
       " 'spark.serializer.objectStreamReset': '100',\n",
       " 'spark.shuffle.service.enabled': 'true',\n",
       " 'spark.shuffle.service.port': '7337',\n",
       " 'spark.sql.catalogImplementation': 'hive',\n",
       " 'spark.sql.hive.metastore.jars': '${env:HADOOP_COMMON_HOME}/../hive/lib/*:${env:HADOOP_COMMON_HOME}/client/*',\n",
       " 'spark.sql.hive.metastore.version': '1.1.0',\n",
       " 'spark.sql.legacy.allowCreatingManagedTableUsingNonemptyLocation': 'true',\n",
       " 'spark.sql.queryExecutionListeners': 'com.cloudera.spark.lineage.NavigatorQueryListener',\n",
       " 'spark.sql.shuffle.partitions': '400',\n",
       " 'spark.sql.warehouse.dir': '/user/hive/warehouse',\n",
       " 'spark.submit.deployMode': 'client',\n",
       " 'spark.task.maxFailures': '15',\n",
       " 'spark.ui.enabled': 'true',\n",
       " 'spark.ui.filters': 'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter',\n",
       " 'spark.ui.killEnabled': 'true',\n",
       " 'spark.ui.port': '4445',\n",
       " 'spark.ui.proxyBase': '/proxy/application_1645879035230_1949',\n",
       " 'spark.ui.showConsoleProgress': 'true',\n",
       " 'spark.yarn.access.namenodes': 'hdfs://nsld3:8020/,hdfs://clsklsbx:8020/',\n",
       " 'spark.yarn.am.extraLibraryPath': '/opt/cloudera/parcels/CDH-5.16.2-1.cdh5.16.2.p0.8/lib/hadoop/lib/native',\n",
       " 'spark.yarn.config.gatewayPath': '/opt/cloudera/parcels',\n",
       " 'spark.yarn.config.replacementPath': '{{HADOOP_COMMON_HOME}}/../../..',\n",
       " 'spark.yarn.dist.jars': '/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hive-jdbc-1.1.0-cdh5.16.2-standalone.jar,/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hive-exec-1.1.0-cdh5.16.2.jar,/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hive-service-1.1.0-cdh5.16.2.jar,/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hive-metastore-1.1.0-cdh5.16.2.jar,/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/commons-configuration-1.6.jar,/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/commons-logging-1.1.3.jar,/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/libthrift-0.9.0.jar,/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/httpclient-4.2.5.jar,/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/httpcore-4.2.5.jar,/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hadoop-auth-2.6.0-cdh5.16.2.jar,/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hadoop-core-2.6.0-mr1-cdh5.16.2.jar,/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hadoop-common-2.6.0-cdh5.16.2.jar',\n",
       " 'spark.yarn.driver.memoryOverhead': '4048mb',\n",
       " 'spark.yarn.executor.memoryOverhead': '8g',\n",
       " 'spark.yarn.historyServer.address': 'http://pklis-chd002181.labiac.df.sbrf.ru:18089',\n",
       " 'spark.yarn.isPython': 'true',\n",
       " 'spark.yarn.jars': 'local:/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/jars/*',\n",
       " 'spark.yarn.keytab': '/home/ektov1-av_ca-sbrf-ru/keytab/user.keytab',\n",
       " 'spark.yarn.principal': 'ektov1-av_ca-sbrf-ru@DF.SBRF.RU',\n",
       " 'spark.yarn.secondary.jars': 'hive-jdbc-1.1.0-cdh5.16.2-standalone.jar,hive-exec-1.1.0-cdh5.16.2.jar,hive-service-1.1.0-cdh5.16.2.jar,hive-metastore-1.1.0-cdh5.16.2.jar,commons-configuration-1.6.jar,commons-logging-1.1.3.jar,libthrift-0.9.0.jar,httpclient-4.2.5.jar,httpcore-4.2.5.jar,hadoop-auth-2.6.0-cdh5.16.2.jar,hadoop-core-2.6.0-mr1-cdh5.16.2.jar,hadoop-common-2.6.0-cdh5.16.2.jar'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(sp.sc.getConf().getAll())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use `SparkSession` as spark runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conf_get = sp.sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7fe048e47390>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = SparkConf().setAll(conf_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sp.sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config('spark.master', 'yarn')\\\n",
    "                            .config('spark.hadoop.hive.metastore.uris', 'thrift://pklis-chd002193.labiac.df.sbrf.ru:48869')\\\n",
    "                            .config('spark.driver.maxResultSize', '90g')\\\n",
    "                            .config('spark.driver.memory', '30g')\\\n",
    "                            .config('spark.executor.cores', '8')\\\n",
    "                            .config('spark.executor.instances', '8')\\\n",
    "                            .config('spark.executor.memory', '25g')\\\n",
    "                            .config('spark.yarn.driver.memoryOverhead', '4048mb')\\\n",
    "                            .config('spark.yarn.executor.memoryOverhead', '8g')\\\n",
    "                            .config('spark.kryoserializer.buffer.max', '1024mb')\\\n",
    "                            .config('spark.dynamicAllocation.enabled', 'false')\\\n",
    "                            .config('spark.ui.port', '4445')\\\n",
    "                            .config('spark.yarn.access.namenodes', 'hdfs://nsld3:8020/,hdfs://clsklsbx:8020/')\\\n",
    "                            .config('spark.sql.hive.metastore.jars', '${env:HADOOP_COMMON_HOME}/../hive/lib/*:${env:HADOOP_COMMON_HOME}/client/*')\\\n",
    "                            .config('spark.yarn.jars', 'local:/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/jars/*')\\\n",
    "                            .config('spark.repl.local.jars', 'file:///opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hive-apache-jdbc-0.13.1-9.jar')\\\n",
    "                            .config('spark.yarn.dist.jars', '/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hive-loaderfactory-02.14.00.00-1.jar, /opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hive-apache-jdbc-0.13.1-9.jar')\\\n",
    "                            .enableHiveSupport().getOrCreate()\n",
    "                            \n",
    "sc= spark.sparkContext\n",
    "hive = HiveContext(sc)                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.eventLog.enabled', 'true'),\n",
       " ('spark.yarn.driver.memoryOverhead', '4048mb'),\n",
       " ('spark.sql.queryExecutionListeners',\n",
       "  'com.cloudera.spark.lineage.NavigatorQueryListener'),\n",
       " ('spark.eventLog.dir', 'hdfs://nsld3/user/spark/spark2ApplicationHistory'),\n",
       " ('spark.ui.port', '4445'),\n",
       " ('spark.serializer', 'org.apache.spark.serializer.KryoSerializer'),\n",
       " ('spark.sql.hive.metastore.jars',\n",
       "  '${env:HADOOP_COMMON_HOME}/../hive/lib/*:${env:HADOOP_COMMON_HOME}/client/*'),\n",
       " ('spark.lineage.log.dir', '/var/log/spark2/lineage'),\n",
       " ('spark.driver.port', '44115'),\n",
       " ('spark.ui.filters',\n",
       "  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " ('spark.driver.memory', '30g'),\n",
       " ('spark.driver.extraLibraryPath',\n",
       "  '/opt/cloudera/parcels/CDH-5.16.2-1.cdh5.16.2.p0.8/lib/hadoop/lib/native'),\n",
       " ('spark.network.crypto.enabled', 'false'),\n",
       " ('spark.driver.host', 'pklis-chd002207.labiac.df.sbrf.ru'),\n",
       " ('spark.ui.enabled', 'true'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.dynamicAllocation.schedulerBacklogTimeout', '1'),\n",
       " ('spark.sql.hive.metastore.version', '1.1.0'),\n",
       " ('spark.yarn.jars',\n",
       "  'local:/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/jars/*'),\n",
       " ('spark.yarn.secondary.jars',\n",
       "  'hive-loaderfactory-02.14.00.00-1.jar,hive-apache-jdbc-0.13.1-9.jar'),\n",
       " ('spark.yarn.dist.jars',\n",
       "  '/opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hive-loaderfactory-02.14.00.00-1.jar, /opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers//hive-apache-jdbc-0.13.1-9.jar'),\n",
       " ('spark.yarn.config.gatewayPath', '/opt/cloudera/parcels'),\n",
       " ('spark.extraListeners', 'com.cloudera.spark.lineage.NavigatorAppListener'),\n",
       " ('spark.driver.appUIAddress',\n",
       "  'http://pklis-chd002207.labiac.df.sbrf.ru:4448'),\n",
       " ('spark.sql.warehouse.dir', '/user/hive/warehouse'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.app.id', 'application_1645879035230_1887'),\n",
       " ('spark.yarn.config.replacementPath', '{{HADOOP_COMMON_HOME}}/../../..'),\n",
       " ('spark.yarn.executor.memoryOverhead', '8g'),\n",
       " ('spark.dynamicAllocation.enabled', 'false'),\n",
       " ('spark.kryoserializer.buffer.max', '1024mb'),\n",
       " ('spark.hadoop.hive.metastore.uris',\n",
       "  'thrift://pklis-chd002193.labiac.df.sbrf.ru:48869'),\n",
       " ('spark.ui.killEnabled', 'true'),\n",
       " ('spark.dynamicAllocation.executorIdleTimeout', '60'),\n",
       " ('spark.yarn.am.extraLibraryPath',\n",
       "  '/opt/cloudera/parcels/CDH-5.16.2-1.cdh5.16.2.p0.8/lib/hadoop/lib/native'),\n",
       " ('spark.io.encryption.enabled', 'false'),\n",
       " ('spark.authenticate', 'false'),\n",
       " ('spark.executorEnv.PYTHONPATH',\n",
       "  '/opt/workspace/ektov1-av_ca-sbrf-ru/libs:/opt/workspace/ektov1-av_ca-sbrf-ru/libs/python3.5/site-packages:/opt/workspace/ektov1-av_ca-sbrf-ru/libs/python3.6/site-packages:/home/ektov1-av_ca-sbrf-ru/python35-libs/lib/python3.5/site-packages:/home/ektov1-av_ca-sbrf-ru/.local/lib/python3.5/site-packages<CPS>/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/lib/py4j-0.10.7-src.zip<CPS>/opt/cloudera/parcels/SPARK2-2.4.0.cloudera2-1.cdh5.13.3.p0.1041012/lib/spark2/python/lib/pyspark.zip'),\n",
       " ('spark.driver.maxResultSize', '90g'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.executor.cores', '8'),\n",
       " ('spark.yarn.access.namenodes', 'hdfs://nsld3:8020/,hdfs://clsklsbx:8020/'),\n",
       " ('spark.yarn.historyServer.address',\n",
       "  'http://pklis-chd002181.labiac.df.sbrf.ru:18089'),\n",
       " ('spark.shuffle.service.enabled', 'true'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  'https://pklis-chd002180.labiac.df.sbrf.ru:8090/proxy/application_1645879035230_1887'),\n",
       " ('spark.executor.instances', '8'),\n",
       " ('spark.executor.extraLibraryPath',\n",
       "  '/opt/cloudera/parcels/CDH-5.16.2-1.cdh5.16.2.p0.8/lib/hadoop/lib/native'),\n",
       " ('spark.shuffle.service.port', '7337'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  'pklis-chd002180.labiac.df.sbrf.ru'),\n",
       " ('spark.app.name', 'pyspark-shell'),\n",
       " ('spark.executor.memory', '25g'),\n",
       " ('spark.lineage.enabled', 'true'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.dynamicAllocation.minExecutors', '0'),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hive-apache-jdbc-0.13.1-9.jar'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `ADD JARS` via HiveContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>databaseName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>default</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>td15_prd_db_client4d_cvm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>td15_prd_db_client4d_dev1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>td15_prd_db_client4d_mis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>td15_sbx_db_sme_dkk_analyse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sbx_core_internal_client4d_mis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sbx_core_internal_client4d_seldon_zakupki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sbx_core_internal_db_dmp4dplg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sbx_core_internal_integrum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sbx_core_internal_pravo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                databaseName\n",
       "0                                    default\n",
       "1                   td15_prd_db_client4d_cvm\n",
       "2                  td15_prd_db_client4d_dev1\n",
       "3                   td15_prd_db_client4d_mis\n",
       "4                td15_sbx_db_sme_dkk_analyse\n",
       "5             sbx_core_internal_client4d_mis\n",
       "6  sbx_core_internal_client4d_seldon_zakupki\n",
       "7              sbx_core_internal_db_dmp4dplg\n",
       "8                 sbx_core_internal_integrum\n",
       "9                    sbx_core_internal_pravo"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hive.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Queries Throught HiveContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Hive-TDQG connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Results='spark://pklis-chd002207.labiac.df.sbrf.ru:43388/jars/hive-exec-1.1.0-cdh5.16.2.jar'),\n",
       " Row(Results='spark://pklis-chd002207.labiac.df.sbrf.ru:43388/jars/hive-metastore-1.1.0-cdh5.16.2.jar'),\n",
       " Row(Results='spark://pklis-chd002207.labiac.df.sbrf.ru:43388/jars/commons-configuration-1.6.jar'),\n",
       " Row(Results='spark://pklis-chd002207.labiac.df.sbrf.ru:43388/jars/hive-jdbc-1.1.0-cdh5.16.2-standalone.jar'),\n",
       " Row(Results='spark://pklis-chd002207.labiac.df.sbrf.ru:43388/jars/libthrift-0.9.0.jar'),\n",
       " Row(Results='spark://pklis-chd002207.labiac.df.sbrf.ru:43388/jars/hadoop-common-2.6.0-cdh5.16.2.jar'),\n",
       " Row(Results='spark://pklis-chd002207.labiac.df.sbrf.ru:43388/jars/terajdbc4.jar'),\n",
       " Row(Results='spark://pklis-chd002207.labiac.df.sbrf.ru:43388/jars/commons-logging-1.1.3.jar'),\n",
       " Row(Results='spark://pklis-chd002207.labiac.df.sbrf.ru:43388/jars/httpcore-4.2.5.jar'),\n",
       " Row(Results='spark://pklis-chd002207.labiac.df.sbrf.ru:43388/jars/tdgssconfig.jar'),\n",
       " Row(Results='spark://pklis-chd002207.labiac.df.sbrf.ru:43388/jars/hadoop-core-2.6.0-mr1-cdh5.16.2.jar'),\n",
       " Row(Results='spark://pklis-chd002207.labiac.df.sbrf.ru:43388/jars/hadoop-auth-2.6.0-cdh5.16.2.jar'),\n",
       " Row(Results='spark://pklis-chd002207.labiac.df.sbrf.ru:43388/jars/hive-service-1.1.0-cdh5.16.2.jar'),\n",
       " Row(Results='spark://pklis-chd002207.labiac.df.sbrf.ru:43388/jars/httpclient-4.2.5.jar')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hive.sql(\"list jars\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DRIVERS_PATH = \"/opt/workspace/{}/notebooks/drivers/\".format(curruser)\n",
    "DRIVERS_ALL = \"/opt/workspace/{}/notebooks/drivers/*\".format(curruser)\n",
    "AUX_JAR_HDFS_PATH = \"hdfs:///user/{}/tdqg/jars/\".format(curruser)\n",
    "AUX_JARS = \"hdfs:///user/{}/tdqg/jars/*\".format(curruser)\n",
    "JAR_PATH = \"/opt/workspace/{}/notebooks/drivers/hive-loaderfactory-02.14.00.00-1.jar\".format(curruser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! hdfs dfs -mkdir -p hdfs:///user/ektov1-av_ca-sbrf-ru/tdqg/jars/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -rm -f -skipTrash hdfs:///user/ektov1-av_ca-sbrf-ru/tdqg/jars/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "! hdfs dfs -put -f /opt/workspace/ektov1-av_ca-sbrf-ru/notebooks/drivers/hive-loaderfactory* hdfs:///user/ektov1-av_ca-sbrf-ru/tdqg/jars/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[result: int]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hive.setConf(\"hive.aux.jars.path\", AUX_JARS)\n",
    "\n",
    "hive.sql(\"ADD JAR {}\".format(JAR_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sdf = hive.table(\"td15_prd_db_client4d_dev1.ga_all_scenarios_hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HiveJdbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Password for ektov1-av_ca-sbrf-ru@DF.SBRF.RU: \r\n"
     ]
    }
   ],
   "source": [
    "!PASS=$(cat /home/$(whoami)/pass/userpswrd | sed 's/\\r//g') && printf \"$PASS\" | kinit $(whoami)@DF.SBRF.RU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100000\n",
    "\n",
    "uri=\"jdbc:hive2://pklis-chd002185.labiac.df.sbrf.ru:10000/sbx_t_team_cvm;principal=hive/_HOST@$DF.SBRF.RU\"\n",
    "\n",
    "sql_query=\"(select * from sbx_t_team_cvm.lal_db_hist_out where create_dt_day='2022-03-24') as t\"\n",
    "\n",
    "sdf=\\\n",
    "hive\\\n",
    "    .read \\\n",
    "    .format('jdbc') \\\n",
    "    .option('url', uri) \\\n",
    "    .option('dbtable', sql_query) \\\n",
    "    .option('driver', 'org.apache.hive.jdbc.HiveDriver') \\\n",
    "    .option('fetchsize', BATCH_SIZE) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sdf.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
